{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Implementing Logistic Regression From Scratch\n",
    "\n",
    "The goal of this notebook is to implement your own logistic regression classifier. You will:\n",
    "\n",
    " * Extract features from Amazon product reviews.\n",
    " * Convert an SFrame into a NumPy array.\n",
    " * Implement the link function for logistic regression.\n",
    " * Write a function to compute the derivative of the log likelihood function with respect to a single coefficient.\n",
    " * Implement gradient ascent.\n",
    " * Given a set of coefficients, predict sentiments.\n",
    " * Compute classification accuracy for the logistic regression model.\n",
    " \n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Import Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Load review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "For this assignment, we will use a subset of the Amazon product review dataset. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews.\n",
    "\n",
    "Load the dataset into a data frame named products. One column of this dataset is sentiment, corresponding to the class label with +1 indicating a review with positive sentiment and -1 for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>Samsung Baby Care Washer, Stainless Platinum, ...</td>\n",
       "      <td>My infant goes to a really crappy daycare, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>Mud Pie Milestone Stickers, Boy</td>\n",
       "      <td>Pretty please open and inspect these stickers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>Best BIB for Baby - Soft Bib (Pink-Elephant)</td>\n",
       "      <td>Great 5-Star Product but An Obvious knock-off ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>Bouncy&amp;reg; Inflatable Real Feel Hopping Cow</td>\n",
       "      <td>When I received the item my initial thought wa...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>Maxboost iPhone 5S/5 Case - Protective Snap-on...</td>\n",
       "      <td>I got this case in the mail today, it came on ...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53072 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "0      Stop Pacifier Sucking without tears with Thumb...   \n",
       "1        Nature's Lullabies Second Year Sticker Calendar   \n",
       "2        Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                            Lamaze Peekaboo, I Love You   \n",
       "4      SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "...                                                  ...   \n",
       "53067  Samsung Baby Care Washer, Stainless Platinum, ...   \n",
       "53068                    Mud Pie Milestone Stickers, Boy   \n",
       "53069       Best BIB for Baby - Soft Bib (Pink-Elephant)   \n",
       "53070       Bouncy&reg; Inflatable Real Feel Hopping Cow   \n",
       "53071  Maxboost iPhone 5S/5 Case - Protective Snap-on...   \n",
       "\n",
       "                                                  review  rating  sentiment  \n",
       "0      All of my kids have cried non-stop when I trie...       5          1  \n",
       "1      We wanted to get something to keep track of ou...       5          1  \n",
       "2      My daughter had her 1st baby over a year ago. ...       5          1  \n",
       "3      One of baby's first and favorite books, and it...       4          1  \n",
       "4      Very cute interactive book! My son loves this ...       5          1  \n",
       "...                                                  ...     ...        ...  \n",
       "53067  My infant goes to a really crappy daycare, and...       1         -1  \n",
       "53068  Pretty please open and inspect these stickers ...       1         -1  \n",
       "53069  Great 5-Star Product but An Obvious knock-off ...       1         -1  \n",
       "53070  When I received the item my initial thought wa...       2         -1  \n",
       "53071  I got this case in the mail today, it came on ...       2         -1  \n",
       "\n",
       "[53072 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = pd.read_csv('amazon_baby_subset.csv')\n",
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let us quickly explore more of this dataset. The name column indicates the name of the product. Try listing the name of the first 10 products in the dataset.\n",
    "\n",
    "After that, try counting the number of positive and negative reviews.\n",
    "\n",
    "**Note:** For this assignment, we eliminated class imbalance by choosing a subset of the data with a similar number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Stop Pacifier Sucking without tears with Thumb...\n",
       "1      Nature's Lullabies Second Year Sticker Calendar\n",
       "2      Nature's Lullabies Second Year Sticker Calendar\n",
       "3                          Lamaze Peekaboo, I Love You\n",
       "4    SoftPlay Peek-A-Boo Where's Elmo A Children's ...\n",
       "5                            Our Baby Girl Memory Book\n",
       "6    Hunnt&reg; Falling Flowers and Birds Kids Nurs...\n",
       "7    Blessed By Pope Benedict XVI Divine Mercy Full...\n",
       "8    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "9    Cloth Diaper Pins Stainless Steel Traditional ...\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['name'].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Apply text cleaning on the review data\n",
    "\n",
    "In this section, we will perform some simple feature cleaning using **data frames**. The last assignment used all words in building bag-of-words features, but here we limit ourselves to 193 words (for simplicity). We compiled a list of 193 most frequent words into the JSON file named **important_words.json**. Load the words into a list **important_words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "important_words = pd.read_json('important_words.json')\n",
    "important_words = important_words.rename({0: 'words'}, axis=1)\n",
    "important_words = list(important_words['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Compute word counts (only for **important_words**)\n",
    "\n",
    "We start with *Step 1* which can be done as follows:\n",
    "\n",
    "* if your tool supports it, fill n/a values in the review column with empty strings. The n/a values indicate empty reviews. For instance, Pandas's the fillna() method lets you replace all N/A's in the review columns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Write a function **remove_punctuation** that takes a line of text and removes all punctuation from that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.translate(str.maketrans('','',string.punctuation)) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Apply the **remove_punctuation** function on every element of the **review** column and assign the result to the new column **review_clean**. Note. Many data frame packages support **apply** operation for this type of task. Consult appropriate manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now we proceed with the *Step 2*. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text.\n",
    "\n",
    "**Note:** There are several ways of doing this. One way is to create an anonymous function that counts the occurrence of a particular word and apply it to every element in the **review_clean column**. Repeat this step for every word in important_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The data frame **products** should contain one column for each of the 193 **important_words**. As an example, the column **perfect** contains a count of the number of times the word **perfect** occurs in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "53067    0\n",
       "53068    0\n",
       "53069    0\n",
       "53070    0\n",
       "53071    0\n",
       "Name: perfect, Length: 53072, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['perfect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now, write some code to compute the number of product reviews that contain the word **perfect**.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "First create a column called `contains_perfect` which is set to 1 if the count of the word **perfect** (stored in column perfect is >= 1.\n",
    "Sum the number of 1s in the column `contains_perfect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda pf: 1 if pf >=1 else 0)\n",
    "contains_perfect = sum(products['contains_perfect'] == 1)\n",
    "contains_perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "variables": {
     "contains_perfect": "2955"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz : How many reviews contain the word <i>perfect</i>? </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : {{contains_perfect}} reviews contain the word <i>perfect</i> </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Convert Data Frame to Multi-Dimensional Array\n",
    "\n",
    "It is now time to convert our data frame to a multi-dimensional array. Look for a package that provides a highly optimized matrix operations. In the case of Python, NumPy is a good choice.\n",
    "\n",
    "Write a function that extracts columns from a data frame and converts them into a multi-dimensional array. We plan to use them throughout the course, so make sure to get this function right.\n",
    "\n",
    "The function should accept three parameters:\n",
    "\n",
    "* **dataframe**: a data frame to be converted\n",
    "* **features**: a list of string, containing the names of the columns that are used as features.\n",
    "* **label**: a string, containing the name of the single column that is used as class labels.\n",
    "\n",
    "\n",
    "The function should return two values:\n",
    "\n",
    "* one 2D array for features\n",
    "* one 1D array for class labels\n",
    "\n",
    "\n",
    "The function should do the following:\n",
    "\n",
    "* Prepend a new **column** constant to **dataframe** and fill it with 1's. This column takes account of the intercept term. Make sure that the constant **column appears** first in the data frame.\n",
    "* Prepend a string `constant` to the list **features**. Make sure the string `constant` appears first in the list.\n",
    "* Extract columns in **dataframe** whose names appear in the list **features**.\n",
    "* Convert the extracted columns into a 2D array using a function in the data frame library. For Pandas, you would use .values function.\n",
    "* Extract the single column in **dataframe** whose name corresponds to the string **label**.\n",
    "* Convert the column into a 1D array.\n",
    "* Return the 2D array and the 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = dataframe[features]\n",
    "    feature_matrix = features_frame.values\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = label_sarray.values\n",
    "    \n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Using the function written above, extract two arrays **feature_matrix** and **sentiment**. The 2D array feature_matrix would contain the content of the columns given by the list **important_words**. The 1D array **sentiment** would contain the content of the column **sentiment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53072 features in the feature_matrix\n",
      "\n",
      "Reation Between number of features in features_matrix(x) and number of features in logistic regression(y) is :\n",
      " y - 121712 = m(x - 53072)\n"
     ]
    }
   ],
   "source": [
    "feature_matrix, sentiment = get_numpy_data(products, important_words, 'sentiment')\n",
    "\n",
    "# The code below is to answer the following quiz questions\n",
    "\n",
    "num_feats_feature_matrix = len(feature_matrix)\n",
    "num_feats_log_reg = 121712\n",
    "\n",
    "print('There are', num_feats_feature_matrix, 'features in the feature_matrix\\n')\n",
    "\n",
    "relation = 'None'\n",
    "\n",
    "if num_feats_log_reg == num_feats_feature_matrix + 1 :\n",
    "    relation = 'y = x + 1'\n",
    "    print('Reation Between number of features in features_matrix(x) and number of features in logistic regression(y) is :' \n",
    "          '\\n', relation)\n",
    "    \n",
    "elif num_feats_log_reg == num_feats_feature_matrix :\n",
    "    relation = 'y = x'\n",
    "    print('Reation Between number of features in features_matrix(x) and number of features in logistic regression(y) is :' \n",
    "          '\\n', relation)\n",
    "\n",
    "elif num_feats_log_reg == num_feats_feature_matrix - 1 :\n",
    "    relation = 'y = x - 1'\n",
    "    print('Reation Between number of features in features_matrix(x) and number of features in logistic regression(y) is :' \n",
    "          '\\n', relation)\n",
    "    \n",
    "else :\n",
    "    relation = str(str('y - ') + str(num_feats_log_reg) + str(' = m(x - ') + str(num_feats_feature_matrix) + str(')'))\n",
    "    print('Reation Between number of features in features_matrix(x) and number of features in logistic regression(y) is :' \n",
    "          '\\n', relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "variables": {
     "num_feats_feature_matrix": "53072",
     "relation": "y - 121712 = m(x - 53072)"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz 1 : How many features are there in `feature_matrix`? </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer 1 : There are {{num_feats_feature_matrix}} features in the feature_matrix </b></font>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<font color='steelblue'><b> Quiz 2 : Assuming that the intercept is present, how does the number of features in `feature_matrix` relate to the number of features in the `logistic regression` model? Let x = number of features in feature_matrix and y = number of features in logistic regression model. </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer 2 : Relation Between number of features in features_matrix(x) and number of features in logistic regression(y) is : </b></font>\n",
    "<font color='slategray'><b> <nbps><nbps><nbps><nbps> {{relation}} </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Estimating conditional probability with link function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Recall from lecture that the link function is given by:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review  $\\mathbf{x}_i$. Write a function named predict_probability that implements the link function.\n",
    "\n",
    "* Take two parameters: **feature_matrix** and **coefficients**.\n",
    "* First compute the dot product of **feature_matrix** and **coefficients**.\n",
    "* Then compute the link function **$P(y=+1|x,w)$**\n",
    "* Return the predictions given by the link function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1 / (1 + np.exp(-score))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Aside**. How the link function works with matrix algebra\n",
    "\n",
    "Since the word counts are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n",
    "$$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n",
    "$$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Checkpoint**\n",
    "\n",
    "The code below should ensure that the `predict_probability` function is correctly implemented. If so, the outputs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_predictions           = [0.98201379 0.26894142]\n",
      "output of predict_probability = [0.98201379 0.26894142]\n"
     ]
    }
   ],
   "source": [
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),          1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_predictions = np.array( [ 1./(1+np.exp(-correct_scores[0])), 1./(1+np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "print('The following outputs must match ')\n",
    "print('------------------------------------------------')\n",
    "print('correct_predictions           =', correct_predictions)\n",
    "print('output of predict_probability =', predict_probability(dummy_feature_matrix, dummy_coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "Recall from lecture:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "We will now write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "* **errors** vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* **feature** vector containing $h_j(\\mathbf{x}_i)$  for all $i$. \n",
    "\n",
    "This corresponds to the j-th column of **feature_matrix**.\n",
    "\n",
    "The function should do the following:\n",
    "\n",
    "* Take two parameters errors and feature.\n",
    "* Compute the dot product of errors and feature.\n",
    "* Return the dot product. This is the derivative with respect to a single coefficient w_j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In the main lecture, our focus was on the likelihood.  In the advanced optional video, however, we introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "Write a function compute_log_likelihood that implements the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores)))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "**Checkpoint**\n",
    "\n",
    "The code below should ensure that the `compute_log_likelihood` function is correctly implemented. If so, the outputs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following outputs must match \n",
      "------------------------------------------------\n",
      "correct_log_likelihood           = -5.331411615436032\n",
      "output of compute_log_likelihood = -5.331411615436032\n"
     ]
    }
   ],
   "source": [
    "dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n",
    "dummy_coefficients = np.array([1., 3., -1.])\n",
    "dummy_sentiment = np.array([-1, 1])\n",
    "\n",
    "correct_indicators  = np.array( [ -1==+1,                                       1==+1 ] )\n",
    "correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),                     1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n",
    "correct_first_term  = np.array( [ (correct_indicators[0]-1)*correct_scores[0],  (correct_indicators[1]-1)*correct_scores[1] ] )\n",
    "correct_second_term = np.array( [ np.log(1. + np.exp(-correct_scores[0])),      np.log(1. + np.exp(-correct_scores[1])) ] )\n",
    "\n",
    "correct_ll          =      sum( [ correct_first_term[0]-correct_second_term[0], correct_first_term[1]-correct_second_term[1] ] ) \n",
    "\n",
    "print('The following outputs must match ')\n",
    "print('------------------------------------------------')\n",
    "print('correct_log_likelihood           =', correct_ll)\n",
    "print('output of compute_log_likelihood =', compute_log_likelihood(dummy_feature_matrix, dummy_sentiment, dummy_coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Taking gradient steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now we are ready to implement our own logistic regression. All we have to do is to write a gradient ascent function that takes gradient steps towards the optimum. \n",
    "\n",
    "Complete the following function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Write a function logistic_regression to fit a logistic regression model using gradient ascent.\n",
    "\n",
    "The function accepts the following parameters:\n",
    "\n",
    "* **feature_matrix**: 2D array of features\n",
    "* **sentiment**: 1D array of class labels\n",
    "* **initial_coefficients**: 1D array containing initial values of coefficients\n",
    "* **step_size**: a parameter controlling the size of the gradient steps\n",
    "* **max_iter**: number of iterations to run gradient ascent\n",
    "\n",
    "The function returns the last set of coefficients after performing gradient ascent.\n",
    "\n",
    "The function carries out the following steps:\n",
    "\n",
    "1. Initialize vector coefficients to **initial_coefficients**.\n",
    "2. Predict the class probability **$P(y=+1|x,w)$** using your predict_probability function and save it to variable **predictions**.\n",
    "3. Compute indicator value for **$(yi=+1)$** by comparing **sentiment** against +1. Save it to variable **indicator**.\n",
    "4. Compute the errors as difference between **indicator** and **predictions**. Save the errors to variable **errors**.\n",
    "5. For each j-th coefficient, compute the per-coefficient derivative by calling **feature_derivative** with the j-th column of **feature_matrix**. Then increment the j-th coefficient by (step_size*derivative).\n",
    "6. Once in a while, insert code to print out the log likelihood.\n",
    "7. Repeat steps 2-6 for **max_iter times**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now, let us run the logistic regression solver with the parameters below:\n",
    "\n",
    "* **feature_matrix** = feature_matrix extracted using get_numpy_data (stored by the same name)\n",
    "* **sentiment** = sentiment extracted using get_numpy_data (stored by the same name)\n",
    "* **initial_coefficients** = a 194-dimensional vector filled with zeros\n",
    "* **step_size** = 1e-7\n",
    "* **max_iter** = 301\n",
    "\n",
    "Save the returned coefficients to variable **coefficients**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "\n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            # YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[:,j])\n",
    "\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            coefficients[j] += step_size * derivative\n",
    "\n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "            \n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13434712\n",
      "iteration   2: log likelihood of observed labels = -36769.35713564\n",
      "iteration   3: log likelihood of observed labels = -36763.58603240\n",
      "iteration   4: log likelihood of observed labels = -36757.82101962\n",
      "iteration   5: log likelihood of observed labels = -36752.06207964\n",
      "iteration   6: log likelihood of observed labels = -36746.30919497\n",
      "iteration   7: log likelihood of observed labels = -36740.56234821\n",
      "iteration   8: log likelihood of observed labels = -36734.82152213\n",
      "iteration   9: log likelihood of observed labels = -36729.08669961\n",
      "iteration  10: log likelihood of observed labels = -36723.35786366\n",
      "iteration  11: log likelihood of observed labels = -36717.63499744\n",
      "iteration  12: log likelihood of observed labels = -36711.91808422\n",
      "iteration  13: log likelihood of observed labels = -36706.20710739\n",
      "iteration  14: log likelihood of observed labels = -36700.50205049\n",
      "iteration  15: log likelihood of observed labels = -36694.80289716\n",
      "iteration  20: log likelihood of observed labels = -36666.39512033\n",
      "iteration  30: log likelihood of observed labels = -36610.01327118\n",
      "iteration  40: log likelihood of observed labels = -36554.19728365\n",
      "iteration  50: log likelihood of observed labels = -36498.93316099\n",
      "iteration  60: log likelihood of observed labels = -36444.20783914\n",
      "iteration  70: log likelihood of observed labels = -36390.00909449\n",
      "iteration  80: log likelihood of observed labels = -36336.32546144\n",
      "iteration  90: log likelihood of observed labels = -36283.14615871\n",
      "iteration 100: log likelihood of observed labels = -36230.46102347\n",
      "iteration 200: log likelihood of observed labels = -35728.89418769\n",
      "iteration 300: log likelihood of observed labels = -35268.51212683\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients=np.zeros(194),\n",
    "                                  step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<font color='steelblue'><b> Quiz : As each iteration of gradient ascent passes, does the log likelihood increase or decrease? </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : As each iteration of gradient ascent passes, the log likelihood increases </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Predicting sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Recall from lecture that class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "\n",
    "* First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n",
    "* Using the formula above, compute the class predictions from the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores :  [ 0.05104571 -0.02936473  0.02411584 ... -0.40986295  0.01411436\n",
      " -0.06755923] \n",
      "\n",
      "Predicted Sentiment :  [ 1 -1  1 ... -1  1 -1] \n",
      "\n",
      "25126 reviews were predicted to have positive sentiment\n"
     ]
    }
   ],
   "source": [
    "scores = np.dot(feature_matrix, coefficients)\n",
    "predicted_sentiment = np.where(scores >=0, 1, -1)\n",
    "print('Scores : ', scores, '\\n\\nPredicted Sentiment : ', predicted_sentiment, '\\n')\n",
    "\n",
    "# The code below is to answer the following quiz question\n",
    "positive_reviews = sum(predicted_sentiment == 1)\n",
    "print(positive_reviews, 'reviews were predicted to have positive sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "variables": {
     "positive_reviews": "25126"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz : How many reviews were predicted to have positive sentiment? </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : {{positive_reviews}} reviews were predicted to have positive sentiment </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "We will now measure the classification accuracy of the model. Recall from the lecture that the classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Reviews correctly classified = 39903\n",
      "# Reviews incorrectly classified = 13169\n",
      "# Reviews total                  = 53072\n",
      "\n",
      "Accuracy of the model :  0.75\n"
     ]
    }
   ],
   "source": [
    "accuracy_log_reg = round(sum(predicted_sentiment == sentiment) / len(sentiment),2)\n",
    "\n",
    "num_mistakes = sum(predicted_sentiment != sentiment)\n",
    "\n",
    "print('# Reviews correctly classified =', len(products) - num_mistakes)\n",
    "print('# Reviews incorrectly classified =', num_mistakes)\n",
    "print('# Reviews total                  =', len(products))\n",
    "\n",
    "print('\\nAccuracy of the model : ', accuracy_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "accuracy_log_reg": "0.75"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz : What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy) </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : What is the accuracy of the model is : {{accuracy_log_reg}} </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which words contribute most to positive & negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in earlier assignment, we were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n",
    "\n",
    "* Treat each coefficient as a tuple, i.e. (**word**, **coefficient_value**).\n",
    "* Sort all the (**word**, **coefficient_value**) tuples by **coefficient_value** in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = list(coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **word_coefficient_tuples** contains a sorted list of (**word**, **coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten \"most positive\" words\n",
    "\n",
    "Compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 words with most positive coefficients are :  ['great', 'love', 'easy', 'little', 'loves', 'well', 'perfect', 'old', 'nice', 'daughter']\n",
      "\n",
      "The word cheap is not present in the top \"most positive words\"\n"
     ]
    }
   ],
   "source": [
    "ten_most_pos_words = [word_coefficient_tuples[i][0] for i in range(10)]\n",
    "print('The 10 words with most positive coefficients are : ', ten_most_pos_words)\n",
    "\n",
    "# The code below is to answer the followin quiz question\n",
    "pos_answers = ['love', 'easy', 'great', 'perfect', 'cheap']\n",
    "not_pos_ans = np.setdiff1d(pos_answers,ten_most_pos_words)\n",
    "not_pos_ans = \", \".join(str(e) for e in not_pos_ans)\n",
    "\n",
    "print('\\nThe word', not_pos_ans, 'is not present in the top \"most positive words\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "not_pos_ans": "cheap"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz : Which word is <u><i>not</i></u> present in the top 10 \"most positive\" words? </b></font>\n",
    "<font color='slategray'><b>\n",
    "- love\n",
    "- easy\n",
    "- great\n",
    "- perfect\n",
    "- cheap  </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : The word <i><u>{{not_pos_ans}}</u></i> is not present in the top \"most positive words </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 words with most negative coefficients are :  ['would', 'product', 'money', 'work', 'even', 'disappointed', 'get', 'back', 'return', 'monitor']\n",
      "\n",
      "The word need is not present in the top \"most negative words\"\n"
     ]
    }
   ],
   "source": [
    "neg_words = word_coefficient_tuples[::-1]\n",
    "ten_most_neg_words = [neg_words[i][0] for i in range(10)]\n",
    "print('The 10 words with most negative coefficients are : ', ten_most_neg_words)\n",
    "\n",
    "# The code below is to answer the followin quiz question\n",
    "neg_answers = ['need', 'work', 'disappointed', 'even', 'return']\n",
    "not_neg_ans = np.setdiff1d(neg_answers,ten_most_neg_words)\n",
    "not_neg_ans = \", \".join(str(e) for e in not_neg_ans)\n",
    "\n",
    "print('\\nThe word', not_neg_ans, 'is not present in the top \"most negative words\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "not_neg_ans": "need"
    }
   },
   "source": [
    "<font color='steelblue'><b> Quiz : Which word is <u><i>not</i></u> present in the top 10 \"most positive\" words? </b></font>\n",
    "<font color='slategray'><b>\n",
    "- need\n",
    "- work\n",
    "- disappointed\n",
    "- even\n",
    "- return  </b></font>\n",
    "\n",
    "<font color='mediumvioletred'><b> Answer : The word <i><u>{{not_neg_ans}}</u></i> is not present in the top \"most negative words </b></font>"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
